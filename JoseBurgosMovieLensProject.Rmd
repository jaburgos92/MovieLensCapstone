---
title: "MovieLens Recommendation Model"
author: "Jose Burgos"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

\newpage

# **Introduction**  

Machine learning models are widely used in data science to predict outcomes based off of existing data. Large datasets can provide a plethora of information that could either be directly used or wrangled in such a way to derive predictors. Statistical methods can then be used with those variables to build simple or complex models for real-world predictions and applications. One such application is in recommendation systems utilized by companies like Netflix to take information from their large databases of user metadata and predict what movies or shows a user may be interested in based on their viewing habits and preferences.

The first capstone project as part of the HarvardX Professional Certificate in Data Science is to utilize R and the publicly available MovieLens movie rating datasets to build a recommendation model that predicts a user's rating of a given movie. There are several datasets available ranging from 100,000 to 32,000,000 ratings that have been standards for machine learning research and projects. The goal of this capstone project is to build a recommendation system using one of the MovieLens datasets with 10,000,000 ratings. The dataset includes three files:

1. "movies.dat" with movie IDs, titles, and genres
2. "ratings.dat" with user IDs, movie IDs, ratings, and timestamps
3. "tags.dat" with user IDs, movie IDs, tags related to the rating, and timestamps
  
For the purposes of this project, only "movies.dat" and "ratings.dat" were joined into a dataset that was then split into a training set ("edx") and a final holdout test set with a ratio of 90/10 per the project prompt and provided code. The goal is to develop a model that uses the "edx" training set to achieve a root mean square error (RMSE) below 0.86490 when applied to the actual ratings in the final holdout test set. 

This report outlines the data cleaning, exploration, and visualizations used to gain insight into variables that could be used to develop a linear regression model. The "edx" training set is then further divided into a training set and a developmental test set in a 90/10 ratio separate from the final holdout test set. Promising variables from exploration are then incorporated one variable at a time with monitoring and discussion of RMSE performance to minimize errors while not overfitting with too many variables. The final model is then applied to the final holdout test set to check if the project goal has been achieved before discussing limitations and future work.

```{r setup, include=FALSE}
library(knitr)
library(kableExtra)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(out.width = "70%")
knitr::opts_chunk$set(out.height = "70%")

# Prompt code as provided by the course to create the "edx" training set and the final_holdout_test set
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)

# MovieLens 10M dataset for reference:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

options(timeout = 120)

dl <- "ml-10M100K.zip"

if(!file.exists(dl))
  download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)
ratings_file <- "ml-10M100K/ratings.dat"

if(!file.exists(ratings_file))
  unzip(dl, ratings_file)
movies_file <- "ml-10M100K/movies.dat"
if(!file.exists(movies_file))
  unzip(dl, movies_file)

ratings <- as.data.frame(str_split(read_lines(ratings_file), fixed("::"), simplify = TRUE),
                         stringsAsFactors = FALSE)
colnames(ratings) <- c("userId", "movieId", "rating", "timestamp")
ratings <- ratings %>%
  mutate(userId = as.integer(userId),
         movieId = as.integer(movieId),
         rating = as.numeric(rating),
         timestamp = as.integer(timestamp))

movies <- as.data.frame(str_split(read_lines(movies_file), fixed("::"), simplify = TRUE),
                        stringsAsFactors = FALSE)
colnames(movies) <- c("movieId", "title", "genres")
movies <- movies %>%
  mutate(movieId = as.integer(movieId))

movielens <- left_join(ratings, movies, by = "movieId")

# Final hold-out test set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
# set.seed(1) # if using R 3.5 or earlier
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in final hold-out test set are also in edx set
final_holdout_test <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from final hold-out test set back into edx set
removed <- anti_join(temp, final_holdout_test)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

\newpage

# **Methods and Analysis**  

The "edx" and final holdout test datasets provided by the course each contain six columns:

1. userId: integers unique to each anonymous rater. Each rater has rated one or move movies.
2. movieId: integers unique to each movie. Each movie has been rated one or more times.
3. rating: a number on a 5-point scale with half-point increments. This is the rating from a unique user rating a unique movie at a unique time.
4. timestamp: an integer that represents the number seconds passed since 12:00am January 1, 1970 on Coordinated Universal Time (UTC).
5. title: a character string that has the movie title with the release year in parentheses appended.
6. genres: a character string that contains a combination of any of 19 genres that describe the movie. Genres include Action, Adventure, Animation, Children's, Comedy, Crime, Documentary, Drama, Fantasy, Film-Noir, Horror, IMAX, Musical, Mystery, Romance, Sci-Fi, Thriller, War, and Western. In cases where a film spans multiple genres, each genre is sorted alphabetically and separated by "|".

These variables can either be directly used or wrangled to extract more information than an integer or string without additional joining of outside datasets. For the purposes of exploration, the "edx" training set is copied into an exploration set that includes new mutated columns that extract multiple pieces of information from existing columns. The new columns for exploration include:

```{r exploration, include=FALSE}
# Make a copy of the "edx" dataset and create new columns derived from the raw columns provided to extract and explore information that could be used as variables in the model 
# Mutations include:
  # rating_week: rating timestamp rounded to the nearest week
  # rating_year: extraction of the year from the rating timestamp to know what year the rating was made
  # movie_year: extraction of the year from the title to know what year the movie was made
  # years_since_release: difference between rating_year and movie_year to know how long after movie release the user rated the movie
  # 19 columns representing if a given genre is present in the genres string
edx_mutated_exp <- edx %>% mutate(
  rating_week = floor_date(as.POSIXct(timestamp, origin = "1970-01-01", tz = "UTC"), "week"), 
  rating_year = as.integer(year(as.POSIXct(timestamp, origin = "1970-01-01", tz = "UTC"))), 
  movie_year =  as.integer(substr(title, nchar(title) - 4, nchar(title) - 1)),
  years_since_release = rating_year - movie_year,
  Action = str_detect(edx$genres, "Action"),
  Adventure = str_detect(edx$genres, "Adventure"),
  Animation = str_detect(edx$genres, "Animation"),
  Children = str_detect(edx$genres, "Children"),
  Comedy = str_detect(edx$genres, "Comedy"),
  Crime = str_detect(edx$genres, "Crime"),
  Documentary = str_detect(edx$genres, "Documentary"),
  Drama = str_detect(edx$genres, "Drama"),
  Fantasy = str_detect(edx$genres, "Fantasy"),
  Film_Noir = str_detect(edx$genres, "Film-Noir"),
  Horror = str_detect(edx$genres, "Horror"),
  IMAX = str_detect(edx$genres, "IMAX"),
  Musical = str_detect(edx$genres, "Musical"),
  Mystery = str_detect(edx$genres, "Mystery"),
  Romance = str_detect(edx$genres, "Romance"),
  Sci_Fi = str_detect(edx$genres, "Sci-Fi"),
  Thriller = str_detect(edx$genres, "Thriller"),
  War = str_detect(edx$genres, "War"),
  Western = str_detect(edx$genres, "Western")
)
edx_mutated_exp <- as.data.frame(edx_mutated_exp)
# Extract the mean of the entire edx dataset for visualizations
mu_exp <- mean(edx_mutated_exp$rating)
```

1. rating_week: a version of the datetime rounded to the nearest week.
2. rating_year: extraction of the year from the rating timestamp.
3. movie_year: extraction of the release year from the movie title.
4. years_since_release: integer representing the number of years between movie release and a rating.
5. Action: binary True or False if "Action" is part of the genres string.
6. Adventure: binary True or False if "Adventure" is part of the genres string.
7. Animation: binary True or False if "Animation" is part of the genres string.
8. Children: binary True or False if "Children" is part of the genres string.
9. Comedy: binary True or False if "Comedy" is part of the genres string.
10. Crime: binary True or False if "Crime" is part of the genres string.
11. Documentary: binary True or False if "Documentary" is part of the genres string.
12. Drama: binary True or False if "Drama" is part of the genres string.
13. Fantasy: binary True or False if "Fantasy" is part of the genres string.
14. Film_Noir: binary True or False if "Film-Noir" is part of the genres string.
15. Horror: binary True or False if "Horror" is part of the genres string.
16. IMAX: binary True or False if "IMAX" is part of the genres string.
17. Musical: binary True or False if "Musical" is part of the genres string.
18. Mystery: binary True or False if "Mystery" is part of the genres string.
19. Romance: binary True or False if "Romance" is part of the genres string.
20. Sci_Fi: binary True or False if "Sci-Fi" is part of the genres string.
21. Thriller: binary True or False if "Thriller" is part of the genres string.
22. War: binary True or False if "War" is part of the genres string.
23. Western: binary True or False if "Western" is part of the genres string.

The general approach to this project is to use linear regression with the effects of factors added one at a time to minimize the root mean square error (RMSE). RMSE is calculated by taking the difference between predicted and actual ratings for each rating, squaring each residual, taking the average of all the squared values, and taking the square root of that value. In R, this error metric can be defined as:

```{r rmse}
RMSE <- function(true_ratings, predicted_ratings){sqrt(mean((true_ratings - predicted_ratings)^2))}
```

\newpage

This approach requires a starting model to build upon. A simple guess is to use the mean of a training set to predict all ratings in a test set. This can be used as a benchmark to see which factors might have enough of a bias from the mean to warrant the addition of a bias term to the initial guess of the training mean. 

The first variable of potential bias is the movies themselves - how well does any given rating of a movie compare to the average rating of that same movie across the entire training set? One way to visualize this is to make a histogram of the average ratings of each movie and compare them to the "edx" set mean, as well as a histogram of how frequent movies with low numbers of ratings compare to those with high numbers of ratings:

```{r movie_graphs, fig.align = "center", echo=FALSE}
# Histogram of the average ratings of each movie compared to the "edx" mean
edx_mutated_exp %>% group_by(movieId) %>% 
  summarize(rating = mean(rating)) %>% 
  ggplot(aes(x = rating)) + 
  geom_histogram(binwidth = 0.1, color = "black") +
  geom_vline(xintercept = mu_exp, color = "black", linetype = "dashed", linewidth = 1) + 
  labs(title = "Distribution of Average Ratings per Movie", x = "Rating", y = "Counts")
# Histogram of total rating counts for each movie on log10 scale
edx_mutated_exp %>% group_by(movieId) %>% 
  summarize(counts = n()) %>% 
  ggplot(aes(x = counts)) +
  coord_trans(x = "log10") + 
  scale_x_continuous(breaks = c(1, 10, 100, 1000, 10000, 20000, 40000)) +
  geom_histogram(binwidth = 1, color = "black") +
  labs(title = "Distribution of Number of Ratings per Movie", x = "Number of Ratings per Movie", y = "Counts")
```

\newpage

Average movie ratings are non-normal but smooth in distribution and skewed negative from the "edx" mean. This implies a potential effect from movieId that could be corrected with a bias term. When numbers of ratings per movie are plotted on a log scale, it is clear that a fraction of movies have thousands of ratings while a substantial number of movies have less than 100 ratings. Rating frequency of a movie may be related to how critically acclaimed it is. This discrepancy should be taken into account in the model.

Another variable of interest is the identity of the user - how well does a user's given rating of a movie compare to the average movie rating of the user of all of their rated movies across the entire training set? It is possible that some raters are more generous and some are more critical with ratings. This bias can be visualized with a histogram of the average ratings of each user compared to the "edx" mean, as well as a histogram of how frequent users with low numbers of ratings compare to those with high numbers of ratings:

```{r user_graphs, fig.align = "center", echo=FALSE}
# Histogram of the average ratings of each user compared to the "edx" mean
edx_mutated_exp %>% group_by(userId) %>% 
  summarize(rating = mean(rating)) %>% 
  ggplot(aes(x = rating)) + 
    geom_histogram(binwidth = 0.1, color = "black") +
    geom_vline(xintercept = mu_exp, color = "black", linetype = "dashed", linewidth = 1) + 
    labs(title = "Distribution of Average Ratings per User", x = "Rating", y = "Counts")
# Histogram of total rating counts for each user on log10 scale
edx_mutated_exp %>% group_by(userId) %>% 
  summarize(counts = n()) %>% 
  ggplot(aes(x = counts)) +
    coord_trans(x = "log10") + 
    scale_x_continuous(breaks = c(1, 10, 50, 100, 500, 1000, 2000, 3000, 5000)) +
    geom_histogram(binwidth = 1, color = "black") +
    labs(title = "Distribution of Number of Ratings per User",  x = "Number of Ratings per User",  y = "Counts")
```

\newpage

Average user ratings are visually normal but shifted positive from the "edx" mean. This implies a potential effect from userId that could be corrected with another bias term. When numbers of ratings per user are plotted on a log scale, it is clear that a subset of users have thousands of ratings while many users have less than 100 ratings. This discrepancy should also be taken into account in the model.

Another raw categorical variable from the provided datasets is the genre column that show the combinations of genres covered by a given film. Some movies have no genres listed, some have one genre, and some have multiple. Each unique combination has its own average rating that could be influenced by what kind of movies individual users prefer. This bias can be visualized with a histogram of the average ratings of each genre combination compared to the "edx" mean:

```{r genres_graph, fig.align = "center", echo=FALSE}
# Histogram of the average ratings of each genre combination compared to the "edx" mean
edx_mutated_exp %>% group_by(genres) %>% 
  summarize(rating = mean(rating)) %>% 
  ggplot(aes(x = rating)) + 
    geom_histogram(binwidth = 0.1, color = "black") +
    geom_vline(xintercept = mu_exp, color = "black", linetype = "dashed", linewidth = 1) + 
    labs(title = "Distribution of Average Ratings per Genre Combination", x = "Rating", y = "Counts")
```

There is non-normal but relatively smooth distribution skewed negative from the "edx" mean. This implies a potential effect from genres that could be corrected with another bias term. 

\newpage

Theoretically, there are 524,288 possible combinations that could arise from the 19 individual genres. Most will be extremely niche and either associated with very few movies or none at all. Other combinations with one or two genres represented will be encountered far more frequently. However, it is possible that there are extra combinations in the training set created by the inclusion of less frequent genres. For the individual components of genres, the counts and average ratings for each can be visualized in a scatter plot:

```{r binary_genres_graph, fig.align = "center", echo=FALSE}
# Scatter plot of the average rating for each genre as a function of number of ratings per genre
edx_mutated_exp %>% 
  # Select only the genre columns and ratings without any additional information
  select(Action, Adventure, Animation, Children, Comedy, Crime, 
         Documentary, Drama, Fantasy, Film_Noir, Horror, IMAX, Musical, 
         Mystery, Romance, Sci_Fi, Thriller, War, Western, rating) %>%
  # Make a table that takes each row and splits it into 19 rows that show a rating, each genre, and whether or not it was present in the row
  # Then concatenate all the new tables together into one big column of rating, genre, and binary_value (TRUE or FALSE)
  pivot_longer(cols = c(Action, Adventure, Animation, Children, Comedy, Crime, 
                        Documentary, Drama, Fantasy, Film_Noir, Horror, IMAX, Musical, 
                        Mystery, Romance, Sci_Fi, Thriller, War, Western), names_to = "genre", values_to = "binary_value") %>%
  # Get average TRUE and FALSE ratings as well as TURE and FALSE total counts (FALSE totals will be 0) for each of the 19 genres
  group_by(genre, binary_value) %>%
  summarize(total_count = sum(binary_value, na.rm = TRUE), avg_rating = mean(rating, na.rm = TRUE),  .groups = "drop") %>%
  pivot_wider(names_from = binary_value, 
              values_from = c(avg_rating, total_count),
              names_glue = "{.value}_{binary_value}",
              names_prefix = "avg_rating_") %>% 
  # Rename the total_count_TRUE and avg_rating_TRUE to remove the unnecessary _TRUE suffixes prior to plotting
  rename(total_count = total_count_TRUE, avg_rating = avg_rating_TRUE) %>% 
  ggplot(aes(x = total_count, y = avg_rating, label = genre)) +
    geom_point() + 
    geom_text(vjust = 1.5, hjust = -0.075, size = 2) + 
    geom_hline(yintercept = mu_exp, color = "black", linetype = "solid", linewidth = 1) + 
    labs(title = "Average Ratings vs Number of Ratings for Each Genre", x = "Number of Ratings", y = "Average Rating")
```

Some genres like IMAX (8181 ratings), Documentary (93066 ratings), and Film-Noir (118571 ratings) are relatively infrequent and have averages that deviate slightly more from the "edx" mean than genres with hundreds of thousands of ratings like Drama, Comedy, and Action. Infrequent genres can create niche genre combinations that could potentially overfit a model with unnecessary categories. This can be investigated by creating a filtered genres column that removes infrequent genres from the provided genre strings. This would reduce the number of unique genre combinations without sacrificing too much predictive data. One pitfall to filtering too many genres is that it is possible that movies that are very different from one another get lumped into a no-genre category that gets its own average rating. 

\newpage

Filtering with as few genres as possible to avoid this, IMAX is chosen as it is the most infrequent genre and is more of a movie format than an actual genre. This can be visualized with a histogram of the average ratings of each filtered genre combination and compare them to the "edx" mean and the raw genres counterpart from before:

```{r filtered_genres_graph, fig.align = "center", echo=FALSE}
# Create a filtered_genres column in the "edx" exploration copy that detects and keeps genres of interest in a string, with undesired genres commented out
edx_mutated_exp %>% mutate(
  filtered_genres = paste0(
    ifelse(str_detect(genres, "Action"), "Action ", ""),
    ifelse(str_detect(genres, "Adventure"), "Adventure ", ""),
    ifelse(str_detect(genres, "Animation"), "Animation ", ""),
    ifelse(str_detect(genres, "Children"), "Children ", ""),
    ifelse(str_detect(genres, "Comedy"), "Comedy ", ""),
    ifelse(str_detect(genres, "Crime"), "Crime ", ""),
    ifelse(str_detect(genres, "Documentary"), "Documentary ", ""),
    ifelse(str_detect(genres, "Drama"), "Drama ", ""),
    ifelse(str_detect(genres, "Fantasy"), "Fantasy ", ""),
    ifelse(str_detect(genres, "Film-Noir"), "Film-Noir ", ""),
    ifelse(str_detect(genres, "Horror"), "Horror ", ""),
    #ifelse(str_detect(genres, "IMAX"), "IMAX ", ""),    # IMAX category commented out from string detection formula
    ifelse(str_detect(genres, "Musical"), "Musical ", ""),
    ifelse(str_detect(genres, "Mystery"), "Mystery ", ""),
    ifelse(str_detect(genres, "Romance"), "Romance ", ""),
    ifelse(str_detect(genres, "Sci-Fi"), "Sci-Fi ", ""),
    ifelse(str_detect(genres, "Thriller"), "Thriller ", ""),
    ifelse(str_detect(genres, "War"), "War ", ""),
    ifelse(str_detect(genres, "Western"), "Western ", "")
  )) %>% 
  # Histogram of the average ratings of each genre combination with "IMAX" filtered out compared to the "edx" mean
  group_by(filtered_genres) %>% 
  summarize(rating = mean(rating)) %>% 
  ggplot(aes(x = rating)) + 
    geom_histogram(binwidth = 0.1, color = "black") +
    geom_vline(xintercept = mu_exp, color = "black", linetype = "dashed", linewidth = 1) + 
    labs(title = "Distribution of Average Ratings per Filtered Genre Combination", x = "Rating", y = "Counts")
```

Filtering out IMAX alone eliminates a few of the outliers greater than the mean to make the distribution closer to normal, despite still skewing negative from the "edx" mean. It could also imply that IMAX as a movie format leads to better reviews. Development of the model will be done with only IMAX filtered out to investigate proof of concept.

Lastly, time dependence can be included in the model based off of the available information. The provided datasets include two sources of time variables: the timestamp of the rating and the release year of the movie. Plotting individual ratings as a function of time would not be effective due to ratings being in discrete half-point steps. To more effectively extract a rating time trend, it would be best to round each timestamp to the nearest week and average each week for time trend evaluation:

```{r rating_week_trend, fig.align = "center", echo=FALSE}
# Scatter plot of the number of ratings per rating week
edx_mutated_exp %>% group_by(rating_week) %>% 
  summarize(rating = mean(rating)) %>% 
  ggplot(aes(x = rating_week, y = rating)) + 
    geom_point() +
    geom_hline(yintercept = mu_exp, color = "black", linetype = "solid", linewidth = 1) + 
    labs(title = "Average Ratings per Rating Week", x = "Rating Week", y = "Average Rating")
```

\newpage

Most weeks have averages that are close to the "edx" average, but the first few years of ratings had a greater variability in average ratings. Movie release years can be extracted from the title strings to similarly plot average rating dependence on release year:

```{r release_year_trends, fig.align = "center", echo=FALSE}
# Scatter plot of average ratings for each year that a movie that was rated was made 
edx_mutated_exp %>% group_by(movie_year) %>% 
  summarize(rating = mean(rating)) %>% 
  ggplot(aes(x = movie_year, y = rating)) +
    geom_point() + 
    scale_x_continuous(breaks = seq(1910, 2010, by = 10)) +
    geom_hline(yintercept = mu_exp, color = "black", linetype = "solid", linewidth = 1) + 
    labs(title = "Average Ratings per Release Year", x = "Release Year", y = "Average Rating")
# Scatter plot of total number of ratings for each year that a movie that was rated was made 
edx_mutated_exp %>% group_by(movie_year) %>% 
  summarize(counts = n()) %>% 
  ggplot(aes(x = movie_year, y = counts)) +
    geom_point() +
    geom_line() + 
    scale_x_continuous(breaks = seq(1910, 2010, by = 10)) +
    labs(title = "Number of Ratings per Release Year", x = "Release Year", y = "Number of Ratings")
```

\newpage

The release years show a clear non-linear trend where older movies tend to be higher rated than newer movies, though there are fewer movies from before 1980 than there are after. Another potentially useful measure of a time effect is to take the difference between a rating year and the release date:

```{r years_since_release_trends, fig.align = "center", echo=FALSE}
# Scatter plot of average ratings as function of number of years between movie release and rating
edx_mutated_exp %>% group_by(years_since_release) %>% 
  summarize(rating = mean(rating)) %>% 
  ggplot(aes(x = years_since_release, y = rating)) +
    geom_point() + 
    scale_x_continuous(breaks = seq(0, 100, by = 10)) +
    geom_hline(yintercept = mu_exp, color = "black", linetype = "solid", linewidth = 1) + 
    labs(title = "Average Ratings per Number of Years Since Release", x = "Years Since Release", y = "Average Rating")
# Scatter plot of number of ratings as function of number of years between movie release and rating
edx_mutated_exp %>% group_by(years_since_release) %>% 
  summarize(counts = n()) %>% 
  ggplot(aes(x = years_since_release, y = counts)) +
    geom_point() +
    geom_line() + 
    scale_x_continuous(breaks = seq(0, 100, by = 10)) +
    labs(title = "Number of Ratings per Number of Years Since Release", x = "Years Since Release", y = "Number of Ratings")
```

This also shows a clear non-linear trend that combines the potential effects of the two available measures of time. However, there are a few instances where the rating timestamp came before the release year indicating some inconsistencies in release years and extreme outliers coming from reviews that were done prior to film release. In order to avoid overfitting with variables that depend on each other, it will be best to ignore the number of years since release and only use the rating timestamp and release year as time-based predictors for this model.

Based on the exploration thus far, it makes sense to build a model around a training mean and iteratively adding biases from the most promising predictors. Movie ID, user ID, genre combination (possibly filtered), movie release year, and rating timestamp have been identified as variables of most interest. 

\newpage

Before building a model, the original "edx" training set should be split into a training and validation test set to be used before the final application to the final holdout test set. It is important to note that the way the original MovieLens datasets are constructed and split by the course guarantees that every userId has at least one rating in both sets, and that every movieId had at least one rating in both sets. All non-duplicates between sets are added back to the "edx" training set solely for model development. The same methodology is used to split the "edx" set where 90% of the data remains in the training set and 10% of the data will become the validation test set, with non-duplicates between sets then being added back to the training set.

```{r validation split, include=FALSE}
# Split "edx" data into training ("train") and validation test ("test") sets, 90/10 ratio
set.seed(1, sample.kind="Rounding") # Seed set for reproducibility of this report and model
edx_test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE)
train <- edx[-edx_test_index,]
temp2 <- edx[edx_test_index,]

# Make sure userId and movieId in "test" set are also in "train" set
test <- temp2 %>% 
  semi_join(train, by = "movieId") %>%
  semi_join(train, by = "userId")

# Add rows removed from "test" set back into "train" set
removed2 <- anti_join(temp2, test)
train <- rbind(train, removed2)

# Remove unnecessary data and values from setup and data exploration
rm(edx, edx_test_index, temp2, removed2, edx_mutated_exp, mu_exp, movies_file, ratings_file)
```

\newpage

# **Results**  

A simple starting point for a model is to assume that the average rating of the entire training set is the predicted rating for the test sets and calculate its RMSE as a baseline model.

```{r mu, include=FALSE}
# Define the project goal RMSE and the mean of "train" set
goal_rmse <- "0.86490"
mu <- mean(train$rating)
# Calculate RMSE of "test" if the mean of "train" is the prediction for all ratings in "test"
mu_rmse <- round(RMSE(test$rating, mu), 5)
```

```{r mu_table, echo=FALSE}
# Create an RMSE results table for different models
# Seed with the course goal and the RMSE of "test" if the mean of "train" is the prediction for all ratings in "test"
rmse_results_table <- data.frame(Model = c("Course Goal RMSE", "Average"), RMSE = c(goal_rmse, mu_rmse)) 
rmse_results_table %>% kable() %>% kable_styling(position = "center", full_width = FALSE)
```

This RMSE is far from the goal of <0.86490, but errors can be added to account for variation. Now that the baseline model of the training set has been established by assuming the average training set rating is the prediction for every rating, the first factor to add to the model is the movieId. Every movie in the validation and final holdout test sets has at least one rating in the training set, and the average rating for each movie can be used to evaluate how strongly each movie deviates from the training set average model. A grouped summary table is created with movieId's as an index, and each movie's average difference between actual rating and the prior model's prediction (training set mean) is used to calculate residuals to minimize and correct. The average rating summary table is then joined to the validation test set to incorporate the movie effect corrections to the prediction. 

```{r movie_bias, include=FALSE}
# Adding movie effects to the average model:
# Make a table of movie-grouped average residuals between ratings and the "train" average to serve as the added movie bias
b_movie_table <- train %>% 
  group_by(movieId) %>% 
  summarize(b_movie = mean(rating - mu))
# Join the movie bias table to "test" set and evaluate RMSE
b_movie <- left_join(test, b_movie_table, by = "movieId") %>%
  mutate(pred = mu + b_movie) %>%
  pull(pred)
# Evaluate RMSE
mu_movie_rmse <- round(RMSE(b_movie, test$rating), 5)
```

```{r mu_movie_table, echo=FALSE}
# Add new model RMSE to RMSE results table for comparison
rmse_results_table <- rmse_results_table %>% rbind(c("Average w/ Movie Effects", mu_movie_rmse))
rmse_results_table %>% kable() %>% kable_styling(position = "center", full_width = FALSE)
```

Incorporation of movie effects is effective in reducing the RMSE closer to the target. Like every movieId, every user in the validation and final holdout test sets has at least one rating in the training set. Its effect can be compounded onto the previous model by making a new grouped summary table with userId's as an index, and each movie's average difference between actual rating and the prior model's prediction (training average plus movie effect bias) to calculate residuals to minimize and correct. This second table is then joined to the validation test set to incorporate the user effect corrections to the prediction. 

```{r user_bias, include=FALSE}
# Adding user effects to the average w/ movie effects model:
# Make a table of user-grouped average residuals between ratings and the "train" average with movie bias predictions to serve as the added user bias
b_user_table <- train %>% 
  left_join(b_movie_table, by='movieId') %>% 
  group_by(userId) %>% 
  summarize(b_user = mean(rating - mu - b_movie))
# Join the user bias table to "test" and evaluate RMSE
b_movie_user <- test %>% 
  left_join(b_movie_table, by = "movieId") %>% 
  left_join(b_user_table, by = "userId") %>% 
  mutate(pred = mu + b_movie + b_user) %>%
  pull(pred)
# Evaluate RMSE
mu_movie_user_rmse <- round(RMSE(b_movie_user, test$rating), 5)
```

```{r mu_movie_user_table, echo=FALSE}
# Add new model RMSE to RMSE results table for comparison
rmse_results_table <- rmse_results_table %>% rbind(c("Average w/ Movie + User Effects", mu_movie_user_rmse))
rmse_results_table %>% kable() %>% kable_styling(position = "center", full_width = FALSE)
```

The addition of user effects to the model is effective in reducing the RMSE closer to the target than the training average and movie effects on their own. Depending on how the "edx" set is randomly split into training and test sets, the RMSE already achieves the goal during testing. However, it is not guaranteed that the RMSE from testing will be the same as with the final holdout test set. The current model may not be robust enough with new data. It is worth exploring the effects of adding a few other predictors to the model, though too many more could lead to overfitting.

Since every movieId and title in the validation and final holdout test sets has at least one rating in the training set, each genre combination from the test sets will also have at least one match in the training set. The effects of genre combinations, whether as-is or filtered, can be incorporated into the model in the same way as with movieId's and userId's. Its effect can be compounded onto the previous model by making a new grouped summary table with genres as an index, and each genre combination's average difference between actual rating and the prior model's prediction (training average plus movie and user effects bias) to calculate residuals to minimize and correct. This third table is then joined to the validation test set (and eventually the final holdout test set) to incorporate the genres effect corrections to the prediction.

```{r genres_bias, include=FALSE}
# Adding genre combination effects to the average w/ movie + user effects model: raw vs filtered
# Raw genres: use genres column as-is without any removal of relatively infrequent genres that may add unnecessary genre combinations
# Make a table of raw genre-grouped average residuals between ratings and the "train" average w/ movie + user bias to serve as the added genres bias
b_genres_table <- train %>% 
  left_join(b_movie_table, by='movieId') %>% 
  left_join(b_user_table, by='userId') %>% 
  group_by(genres) %>% 
  summarize(b_genres = mean(rating - mu - b_movie - b_user))
# Join the raw genre combination bias table to "test" and evaluate RMSE
b_movie_user_genres <- test %>% 
  left_join(b_movie_table, by = "movieId") %>% 
  left_join(b_user_table, by = "userId") %>% 
  left_join(b_genres_table, by = "genres") %>% 
  mutate(pred = mu + b_movie + b_user + b_genres) %>%
  pull(pred)
# Evaluate RMSE
mu_movie_user_genres_rmse <- round(RMSE(b_movie_user_genres, test$rating), 5)
```

```{r mu_movie_user_genres_table, echo=FALSE}
# Add new model RMSE to RMSE results table for comparison
rmse_results_table <- rmse_results_table %>% rbind(c("Average w/ Movie + User + Genres Effects", mu_movie_user_genres_rmse))
rmse_results_table %>% kable() %>% kable_styling(position = "center", full_width = FALSE)
```

Incorporation of the genres effects is modestly effective in reducing the RMSE compared to the previous model. With 19 genres being combined into over 700 combinations, some combinations will be very niche with only a few representations, as opposed to the most common combinations with hundreds of thousands of representations. As discussed during exploration, it might make sense to trim off references to IMAX as it is infrequent and more of a format than a genre. In order to evaluate if filtering is a good idea, the "edx" training and test sets have a new filtered genre column added that removes IMAX from the character strings. Its effect can be compounded onto the previous model by making a new grouped summary table with filtered genres as an index, and each filtered genre combination's average difference between actual rating and the prior model's prediction (training average plus movie and user effects bias) to calculate residuals to minimize and correct. A filtered model removing just one infrequent genre can be applied to see if it improves RMSE more than using the genres as-is.

```{r filtered_genre_bias, include=FALSE}
# Filtered genres: "train" and "test" sets need a new filtered genre combination column that filters out IMAX from the genres column
train <- train %>% mutate(
  filtered_genres = paste0(
    ifelse(str_detect(genres, "Action"), "Action ", ""),
    ifelse(str_detect(genres, "Adventure"), "Adventure ", ""),
    ifelse(str_detect(genres, "Animation"), "Animation ", ""),
    ifelse(str_detect(genres, "Children"), "Children ", ""),
    ifelse(str_detect(genres, "Comedy"), "Comedy ", ""),
    ifelse(str_detect(genres, "Crime"), "Crime ", ""),
    ifelse(str_detect(genres, "Documentary"), "Documentary ", ""),
    ifelse(str_detect(genres, "Drama"), "Drama ", ""),
    ifelse(str_detect(genres, "Fantasy"), "Fantasy ", ""),
    ifelse(str_detect(genres, "Film-Noir"), "Film-Noir ", ""),
    ifelse(str_detect(genres, "Horror"), "Horror ", ""),
    #ifelse(str_detect(genres, "IMAX"), "IMAX ", ""),    # IMAX category commented out from string detection formula
    ifelse(str_detect(genres, "Musical"), "Musical ", ""),
    ifelse(str_detect(genres, "Mystery"), "Mystery ", ""),
    ifelse(str_detect(genres, "Romance"), "Romance ", ""),
    ifelse(str_detect(genres, "Sci-Fi"), "Sci-Fi ", ""),
    ifelse(str_detect(genres, "Thriller"), "Thriller ", ""),
    ifelse(str_detect(genres, "War"), "War ", ""),
    ifelse(str_detect(genres, "Western"), "Western ", "")
  ))
train <- as.data.frame(train)
test <- test %>% mutate(
  filtered_genres = paste0(
    ifelse(str_detect(genres, "Action"), "Action ", ""),
    ifelse(str_detect(genres, "Adventure"), "Adventure ", ""),
    ifelse(str_detect(genres, "Animation"), "Animation ", ""),
    ifelse(str_detect(genres, "Children"), "Children ", ""),
    ifelse(str_detect(genres, "Comedy"), "Comedy ", ""),
    ifelse(str_detect(genres, "Crime"), "Crime ", ""),
    ifelse(str_detect(genres, "Documentary"), "Documentary ", ""),
    ifelse(str_detect(genres, "Drama"), "Drama ", ""),
    ifelse(str_detect(genres, "Fantasy"), "Fantasy ", ""),
    ifelse(str_detect(genres, "Film-Noir"), "Film-Noir ", ""),
    ifelse(str_detect(genres, "Horror"), "Horror ", ""),
    #ifelse(str_detect(genres, "IMAX"), "IMAX ", ""),    # IMAX category commented out from string detection formula
    ifelse(str_detect(genres, "Musical"), "Musical ", ""),
    ifelse(str_detect(genres, "Mystery"), "Mystery ", ""),
    ifelse(str_detect(genres, "Romance"), "Romance ", ""),
    ifelse(str_detect(genres, "Sci-Fi"), "Sci-Fi ", ""),
    ifelse(str_detect(genres, "Thriller"), "Thriller ", ""),
    ifelse(str_detect(genres, "War"), "War ", ""),
    ifelse(str_detect(genres, "Western"), "Western ", "")
  )) 
test <- as.data.frame(test)
# Make a table of filtered genre-grouped average residuals between ratings and the "train" set average w/ movie + user bias to serve as the added filtered genres bias
b_filtered_genres_table <- train %>% 
  left_join(b_movie_table, by='movieId') %>% 
  left_join(b_user_table, by='userId') %>% 
  group_by(filtered_genres) %>% 
  summarize(b_filtered_genres = mean(rating - mu - b_movie - b_user))
# Join the filtered genre combination bias table to "test" and evaluate RMSE
b_movie_user_filtered_genres <- test %>% 
  left_join(b_movie_table, by = "movieId") %>% 
  left_join(b_user_table, by = "userId") %>% 
  left_join(b_filtered_genres_table, by = "filtered_genres") %>% 
  mutate(pred = mu + b_movie + b_user + b_filtered_genres) %>%
  pull(pred)
# Evaluate RMSE
mu_movie_user_filtered_genres_rmse <- round(RMSE(b_movie_user_filtered_genres, test$rating), 5)
```

```{r mu_movie_user_filtered_genres_table, echo=FALSE}
# Add new model RMSE to RMSE results table for comparison
rmse_results_table <- rmse_results_table %>% rbind(c("Average w/ Movie + User + Filtered Genres Effects", mu_movie_user_filtered_genres_rmse))
rmse_results_table %>% kable() %>% kable_styling(position = "center", full_width = FALSE)
```

Filtering genres for even one uncommon genre actually increases the RMSE in the validation test set. To avoid overcomplicating the model, the raw genres will be used as-is with references to IMAX. The addition of genre effects to the mean + user effects + movie effects model does make a small improvement, so it is kept in the model. 

```{r mu_movie_user_genres_table_fix, echo=FALSE}
# The table shows that filtering is not effective compared to using genres as-is
# Filtered genres-related results not needed going forwards and can be removed from the results table
rmse_results_table <- rmse_results_table[-nrow(rmse_results_table), ]
rm(b_filtered_genres_table, b_movie_user_filtered_genres, mu_movie_user_filtered_genres_rmse)
```

One more factor to consider is the time effects of when a movie was released and/or rated. Every user and every movie in the validation and final holdout test sets has at least one rating in the training set. This also means that every discrete release year as derived from the movie title has at least one match in the training set. The approach that has been used of joining tables of average bias per grouped movie, user, or genre combination can also be applied. However, the same guarantee does not necessarily hold for rating timestamps or time between rating and release. The same approach of creating a table grouped by movie years to join onto the model can be tried with the release year to see if there is further improvement.

```{r movie_year_bias, include=FALSE}
# Adding release year effects to the average w/ movie + user + genres effects model:
# "Train" and "test" sets need a new movie year column that extracts release year from movie titles
train <- train %>% mutate(movie_year =  as.integer(substr(title, nchar(title) - 4, nchar(title) - 1))) 
train <- as.data.frame(train)
test <- test %>% mutate(movie_year =  as.integer(substr(title, nchar(title) - 4, nchar(title) - 1))) 
test <- as.data.frame(test)
# Make a table of release year-grouped average residuals between ratings and the "train" set average w/ movie + user + genres bias to serve as the added movie year bias
b_movie_year_table <- train %>% 
  left_join(b_movie_table, by='movieId') %>% 
  left_join(b_user_table, by='userId') %>% 
  left_join(b_genres_table, by='genres') %>% 
  group_by(movie_year) %>% 
  summarize(b_movie_year = mean(rating - mu - b_movie - b_user - b_genres))
# Join the movie year bias table to "test" and evaluate RMSE
b_movie_user_genres_movie_year <- test %>% 
  left_join(b_movie_table, by = "movieId") %>% 
  left_join(b_user_table, by = "userId") %>% 
  left_join(b_genres_table, by = "genres") %>% 
  left_join(b_movie_year_table, by = "movie_year") %>% 
  mutate(pred = mu + b_movie + b_user + b_genres + b_movie_year) %>%
  pull(pred)
# Evaluate RMSE
mu_movie_user_genres_movie_year_rmse <- round(RMSE(b_movie_user_genres_movie_year, test$rating), 5)
```

```{r mu_movie_user_genres_movie_year_table, echo=FALSE}
# Add new model RMSE to RMSE results table for comparison
rmse_results_table <- rmse_results_table %>% rbind(c("Average w/ Movie + User + Genres + Release Year Effects", mu_movie_user_genres_movie_year_rmse))
rmse_results_table %>% kable() %>% kable_styling(position = "center", full_width = FALSE)
```

The addition of the release year effects is effective in further reducing the RMSE. An additional technique to try is regularization of the model, which adds a penalty to large estimates of effect biases when the sample sizes used to calculate the estimates are small compared to other estimates. This should be effective as the prior exploration shows that movie, users, genres, and release years all have averages from samplings that range from a few counts to hundreds of thousands of counts. The average with movie + user + genres + release year effects model can be regularized all at once with cross validation of penalty lambda values.

```{r mu_reg_movie_user_genres_movie_year_bias, fig.align = "center", echo=FALSE}
# Regularizing the "train" average w/ movie + user + genres + release year effects model:
# Define a set of penalized least squares lambdas for cross-validation
lambdas <- seq(0, 6, 0.2)
# Apply regularization to the model and create a table of RMSE's for each cross-validated lambda
rmses <- sapply(lambdas, function(lambda){
  # Make a table of movie-grouped penalized average residuals between ratings and the "train" set average to serve as the added regularized movie bias
  b_reg_movie_table <- train %>%
    group_by(movieId) %>%
    summarise(b_reg_movie = sum(rating - mu) / (n() + lambda))
  # Make a table of user-grouped penalized average residuals between ratings and the "train" average w/ regularized movie effects to serve as the added regularized user bias
  b_reg_user_table <- train %>%
    left_join(b_reg_movie_table, by="movieId") %>%
    group_by(userId) %>%
    summarise(b_reg_user = sum(rating - mu - b_reg_movie) / (n() + lambda))
  # Make a table of genres-grouped penalized average residuals between ratings and "train" average w/ regularized movie + user bias to serve as the added regularized genres bias
  b_reg_genres_table <- train %>%
    left_join(b_reg_movie_table, by="movieId") %>%
    left_join(b_reg_user_table, by="userId") %>%
    group_by(genres) %>%
    summarise(b_reg_genres = sum(rating - mu - b_reg_movie - b_reg_user) / (n() + lambda))
  # Make a table of movie year-grouped penalized average residuals between ratings and "train" average w/ regularized movie + user + genres bias to serve as the added movie year bias
  b_reg_movie_year_table <- train %>%
    left_join(b_reg_movie_table, by="movieId") %>%
    left_join(b_reg_user_table, by="userId") %>%
    left_join(b_reg_genres_table, by="genres") %>%
    group_by(movie_year) %>%
    summarise(b_reg_movie_year = sum(rating - mu - b_reg_movie - b_reg_user - b_reg_genres) / (n() + lambda))
  # Join the regularized bias tables to the "test" set and evaluate RMSE
  b_reg_movie_user_genres_movie_year <- test %>%
    left_join(b_reg_movie_table, by="movieId") %>%
    left_join(b_reg_user_table, by="userId") %>%
    left_join(b_reg_genres_table, by="genres") %>%
    left_join(b_reg_movie_year_table, by="movie_year") %>%
    mutate(b_reg_movie_user_genres_movie_year = mu + b_reg_movie + b_reg_user + b_reg_genres + b_reg_movie_year) %>%
    pull(b_reg_movie_user_genres_movie_year)
  # Evaluate RMSE
  return(RMSE(b_reg_movie_user_genres_movie_year, test$rating))
})
reg_lambdas <- data.frame(lambdas, rmses)

# Identify the lambda associated with the lowest RMSE
best_lambda <- lambdas[which.min(rmses)]

# Scatter plot of regularized RMSE's as a function of each lambda screened
reg_lambdas %>% ggplot(aes(x = lambdas, y = rmses)) +
  geom_point() +
  geom_line() + 
  scale_x_continuous(breaks = seq(0, 6, by = 0.5)) +
  geom_vline(xintercept = best_lambda, color = "black", linetype = "solid", linewidth = 1) + 
  labs(title = "Optimal Lambda for Model Regularization", x = "Lambda", y = "RMSE")

# Apply best lambda with the minimized RMSE to the regularized model
# Make a table of movie-grouped penalized average residuals between ratings and the "train" set average to serve as the added regularized movie bias
b_reg_movie_table <- train %>%
  group_by(movieId) %>%
  summarise(b_reg_movie = sum(rating - mu) / (n() + best_lambda))
# Make a table of user-grouped penalized average residuals between ratings and the "train" average w/ regularized movie effects to serve as the added regularized user bias
b_reg_user_table <- train %>%
  left_join(b_reg_movie_table, by="movieId") %>%
  group_by(userId) %>%
  summarise(b_reg_user = sum(rating - mu - b_reg_movie) / (n() + best_lambda))
# Make a table of genres-grouped penalized average residuals between ratings and "train" average w/ regularized movie + user bias to serve as the added regularized genres bias
b_reg_genres_table <- train %>%
  left_join(b_reg_movie_table, by="movieId") %>%
  left_join(b_reg_user_table, by="userId") %>%
  group_by(genres) %>%
  summarise(b_reg_genres = sum(rating - mu - b_reg_movie - b_reg_user) / (n() + best_lambda))
# Make a table of movie year-grouped penalized average residuals between ratings and "train" average w/ regularized movie + user + genres bias to serve as the added movie year bias
b_reg_movie_year_table <- train %>%
  left_join(b_reg_movie_table, by="movieId") %>%
  left_join(b_reg_user_table, by="userId") %>%
  left_join(b_reg_genres_table, by="genres") %>%
  group_by(movie_year) %>%
  summarise(b_reg_movie_year = sum(rating - mu - b_reg_movie - b_reg_user - b_reg_genres) / (n() + best_lambda))
# Join the regularized bias tables to the "test" set and evaluate RMSE
b_reg_movie_user_genres_movie_year <- test %>%
  left_join(b_reg_movie_table, by="movieId") %>%
  left_join(b_reg_user_table, by="userId") %>%
  left_join(b_reg_genres_table, by="genres") %>%
  left_join(b_reg_movie_year_table, by="movie_year") %>%
  mutate(b_reg_movie_user_genres_movie_year = mu + b_reg_movie + b_reg_user + b_reg_genres + b_reg_movie_year) %>%
  pull(b_reg_movie_user_genres_movie_year)
# Evaluate RMSE
mu_reg_movie_user_genres_movie_year_rmse <- round(RMSE(b_reg_movie_user_genres_movie_year, test$rating), 5)
```

```{r mu_reg_movie_user_genres_movie_year_table, echo=FALSE}
# Add regularized RMSE to RMSE results table for comparison
rmse_results_table <- rmse_results_table %>% rbind(c("Average w/ Regularized Movie + User + Genres + Release Year Effects", mu_reg_movie_user_genres_movie_year_rmse))
rmse_results_table %>% kable() %>% kable_styling(position = "center", full_width = FALSE)
# With regularization shown to be effective, unregularized bias tables can be removed
rm(b_movie_table, b_user_table, b_genres_table, b_movie_year_table, b_movie, b_movie_user, b_movie_user_genres, b_movie_user_genres_movie_year)
```

Regularization using the optimal least squares penalty lambda identified though cross-validation is effective in further reducing the RMSE. The last readily available predictor to investigate is the rating timestamp, which is a continuous variable. Since timestamps or rounded timestamp weeks do not always have matches between the training and test sets, the approach of constructing tables to join onto test sets will not work. An alternative way to incorporate the rating timestamp into the model is to calculate residuals between actual ratings and predicted ratings from the regularized model and fit a line as a function of the rating timestamp. This fitted line represents a formula that can be applied to any given timestamp for generating a residual correction to add to the other bias terms in the model. Once a slope and intercept from the fit have been calculated and used to derive a formula column in the test sets, a final RMSE can be evaluated before choosing a final model for the final holdout test set.

\newpage

```{r timestamp_residual_fit, echo=FALSE}
# Adding rating timestamp effects to the average w/ regularized movie + user + genres + movie year effects model:
# Since timestamps are continuous and do not have guaranteed matches between "train" and "test" sets, grouped average table joining approach will not work
# Instead, make a column in the "train" set of residuals between actual ratings and predicted ratings from the regularized model
train <- train %>% 
  left_join(b_reg_user_table, by = 'userId') %>%
  left_join(b_reg_movie_table, by = 'movieId') %>%
  left_join(b_reg_genres_table, by = 'genres') %>%
  left_join(b_reg_movie_year_table, by = 'movie_year') %>%
  mutate(residual = rating - mu - b_reg_movie - b_reg_user - b_reg_genres - b_reg_movie_year)
# Apply a linear fit between residuals and rating timestamps and define the slope and intercept
timestamp_all_coefs <- coef(lm(train$residual ~ train$timestamp))
timestamp_slope <- as.numeric(timestamp_all_coefs[2])
timestamp_intercept <- as.numeric(timestamp_all_coefs[1])
# Join the regularized bias tables to the "test" set
# Create a timestamp residual column using the slope and intercept 
# Create a predicted rating column that adds a given rating's predicted timestamp residual to the "train" set average and regularized effects
test <- test %>%
  left_join(b_reg_movie_table, by="movieId") %>%
  left_join(b_reg_user_table, by="userId") %>%
  left_join(b_reg_genres_table, by="genres") %>%
  left_join(b_reg_movie_year_table, by="movie_year") %>% mutate(
    timestamp_residual = (timestamp_slope * timestamp) + timestamp_intercept,
    pred_rating =  mu + b_reg_user + b_reg_movie + b_reg_genres + b_reg_movie_year + timestamp_residual)
# Evaluate RMSE
mu_reg_movie_user_genres_movie_year_res_timestamp_rmse <- round(RMSE(test$rating, test$pred_rating), 5)
```

```{r mu_reg_movie_user_genres_movie_year_res_timestamp_table, echo=FALSE}
# Add new model RMSE to RMSE results table for comparison
rmse_results_table <- rmse_results_table %>% rbind(c("Average w/ Regularized Movie + User + Genres + Release Year Effects & Timestamp Residual Fit", mu_reg_movie_user_genres_movie_year_res_timestamp_rmse))
rmse_results_table %>% kable() %>% kable_styling(position = "center", full_width = FALSE)
```

This model has shown to have the lowest RMSE. When applied to the final holdout test set after deriving a release year column from the movie titles for joining the release year bias, the model is also successful in delivering an RMSE below the course goal of 0.86490.

```{r final_holdout, echo=FALSE}
# Final holdout test set needs a new movie year column that extracts release year from movie titles for the movie year component of the final model to join onto
final_holdout_test_model <- final_holdout_test %>% mutate(movie_year =  as.integer(substr(title, nchar(title) - 4, nchar(title) - 1))) 
# Join the regularized bias tables to the test set
# Create an estimated timestamp residual column using the "train" set fitted slope and intercept formula
# Create a predicted rating column summing the "train" average, regularized biases, and estimated timestamp residual
final_holdout_test_pred <- final_holdout_test_model %>%
  left_join(b_reg_movie_table, by = "movieId") %>% 
  left_join(b_reg_user_table, by = "userId") %>% 
  left_join(b_reg_genres_table, by = "genres") %>% 
  left_join(b_reg_movie_year_table, by = "movie_year") %>% mutate(
    timestamp_residual = (timestamp_slope * timestamp) + timestamp_intercept,
    pred_rating = mu + b_reg_movie + b_reg_user + b_reg_genres + b_reg_movie_year + timestamp_residual)
# Evaluate RMSE
final_holdout_test_rmse <- round(RMSE(final_holdout_test_pred$pred_rating, final_holdout_test$rating), 5)
```

```{r final_table, echo=FALSE}
# Table to verify final holdout test set RMSE is below the course goal RMSE
final_rmse_results_table <- data.frame(Model = c("Course Goal RMSE", "Best Model Training RMSE", "Best Model Final Holdout Set RMSE"), RMSE = c(goal_rmse, mu_reg_movie_user_genres_movie_year_res_timestamp_rmse, final_holdout_test_rmse)) 
final_rmse_results_table %>% kable() %>% kable_styling(position = "center", full_width = FALSE)
```

\newpage

# **Conclusion**  

The goal of this capstone project to build a recommendation model that predicts a user's rating of a given movie in a final holdout test set with an RMSE of 0.86490 using several linear regression techniques. Initial exploration of the provided data shows that movieId, userId, genres, movie release years as derived from movie titles, and rating timestamps could be the most promising predictors for a linear regression model. 

The method of dividing the "edx" training set into 90% training and 10% developmental test sets (with users or movies not found in both tables being put only into the training set) is the same method as was done to split the "edx" training set from the final holdout test set. This ensures that all movieIds, userIds, genres, titles, and release years found in the test sets are found in the training set as well. The way these sets are split also enables a modeling approach of using the training mean as the prediction and adding predictor bias effects one at a time. For most identified predictors, a grouped summary table of average deviations between actual rating and the prior model's prediction is created and joined onto test sets as an additional bias term. Cross-validated regularization is applied to the model that uses the training average with movie, user, genres, and release year effects accounted for to suppress outlier effects from movies not often rated, users who rate infrequently, genre combinations that are too unique, and release years with few movies rated. The last refinement to the model is the incorporation of the rating timestamp effects. This involves fitting a line to the remaining residuals in the training set as a function of timestamp and using the line to formulate a predicted offset from rating timestamp effects that can be applied to any timestamp in the test sets. This final model is the end product of the project.

A histogram of the remaining residuals in the final holdout test set can help illustrate if further refinements could improve the RMSE.

```{r final_residual_histogram, fig.align = "center", echo=FALSE}
# Histogram of the final holdout test prediction residuals
final_holdout_test_pred %>% mutate(residual = pred_rating - rating) %>%
  ggplot(aes(x = residual)) + 
    geom_histogram(binwidth = 0.1, color = "black") +
    geom_vline(xintercept = 0, color = "black", linetype = "dashed", linewidth = 1) + 
    labs(title = "Distribution of Final Holdout Test Residuals from Final Model", x = "Residual", y = "Counts")
```

The peak of the residual distribution is very close to the ideal of zero but shifted slightly negative, and the shape is slightly skewed positive. Future work would involve finding ways to tighten the distribution, make the curve more normal, and bring the peak closer to zero. The provided data sets are limited in information that could be used as predictors, but it is possible to wrangle other predictive information that may be effective without overfitting. Filtering genres does not seem to work when trialing removal of "IMAX", but it is possible that optimizing the right genre(s) to filter out or finding some other ways to transform the genre combinations into effective predictors could improve the model.

Due to the way the model is constructed, it is limited only to predicting ratings for users and movies that are both found in the training set. Any new users, movies, genres, and release years would not be able to have the model applied as-is, and the new ratings would need to be included in a retraining of the model. The rating timestamp effects component of the model offers the flexibility of interpolation for ratings within the timespan that the model is trained on, but it is unclear how a linear approximation would extrapolate to newer review timestamp years after the last of the fitted timestamps. Future work could also be done on making this predictor more robust than a linear fit on what may be a non-linear best fit. The overall linear regression approach is effective for the size of the datasets used in this project on a home computer, but there are many more sophisticated machine learning algorithms left to explore that could result in better predictions.
